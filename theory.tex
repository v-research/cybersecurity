In order to address the problem raised by Herley, 
we shall define how to distinguish between a secure and an insecure system.
While most of the literature (and this paper so far) have
correlated the problem of in-security to the maliciousness of
agents interacting with the system, we show that security
doesn't seem to have a malicious nature but, rather, insecurity 
raises from the lack of well-defined security requirements for the
design process of a system. We argue that the high number
of security vulnerability reported today are simply the realization
of potential system configurations, which deviate from the nominal
behavior just because the intended behavior of the system is not precisely 
defined in the specification of the design process. 

Given that we apply our theory to \emph{systems engineering} and that
the two foundational processes (exploitation and hacking) are based on the
concept of a system, we shall begin by defining what a system is, in its
abstract form.

\subsection{Abstract System State}\label{sec:systemstate}
The ISO/IEC/IEEE 15288:2015 (System Life Cycle Processes) provides a definition
of system as ``A combination of interacting elements organized to achieve one
or more stated purposes.''\autocite{ISO201515288}.  Therefore, a system can be
considered as a single agent where its interacting elements are the
constituents of the agent itself. For the sake of simplicity we first define a
system as an agent and then extend the definition to a ``combination of
interacting'' agents.  We use the same process of \autocite{Santaca2016abf} but
over slightly different terms. The differences and commonalities with
\autocite{Santaca2016abf} are described in Appendix~\ref{app:abf}. 

There is no agreement between the research communities (e.g.
Multi-Agent-System, Epistemic Logic) on which are the constituent of an agent
as a system. However, the same ideas revolved around for thousands of years.
Some examples follows.
\begin{itemize}
	\item In \autocite{Hintikka1962knowledge}, Hintikka describes the
		difference between Knowledge and Belief (as epistemological
		concepts), and the whole Doxastic logic defines in details how
		Beliefs can be formalized.
	\item In \autocite{Hintikka1993Information}, Hintikka describes the concept
		of Information and the difference with Knowledge and Belief.
	\item In \autocite{Empiricus1990Pyrrhonism}, the author states: ``The
		logical criterion also may be used in three senses -- of the
		agent, or the instrument, or the ``according to what''; the
		agent, for instance, may be a man, the instrument either sense
		perception or intelligence, and the ``according to what'' the
		application of the impression ``according to'' which the man
		proceeds to judge by means of one of the aforesaid
		instruments.'' 
	\item In \autocite{Santaca2016abf}, the authors defines an agent as a
		tuple of Assertions, Beliefs, and Facts.
\end{itemize}

If we consider \autocite{Empiricus1990Pyrrhonism}, the author applies a
``logical criterion'' to an agent, for instance a man, for instance a device in
a system (or a system) as in \autocite{Santaca2016abf}.  The instrument identified is sense
perception, because a man is considered as example, but we can abstract the
concept of instrument as any received information, where information is
considered as defined in \autocite{Hintikka1993Information}; which can be
summarized by the following axiom: ``Information is specified by specifying
which alternatives concerning the reality it admits and which alternatives
excludes''. This means, for our argument, that considering a propositional
variable, which admits the two alternatives True/False, its information is
defined as Believed to be True/False and not Believed to be the opposite. 
%Similarly, when reasoning on a
%mereotopological region w.r.t. the RCC (e.g. RCC3 admits two regions to be
%equal, disjoint, or overlapping) information on this region should exclude
%some of the alternatives. 
Finally, in \autocite{Empiricus1990Pyrrhonism}
the author states that the logical criterium can be judged according to a frame of reference.
For our argument, as in \autocite{Santaca2016abf}, we define this concept
as Knowledge (or Facts); where knowledge is defined, as in \autocite{Steup2020epistemology}, as
a set of proposition known by an agent, such that: (i) knowledge requires belief,
(ii) knowledge require truth, (iii) knowledge must be \emph{properly justified}.

Before detailing what ``properly justified'' means, we discuss the mentioned
relation between agent and belief and we introduce a more formal definition
of knowledge and belief. For our argument, the only objective of
Information is to exchange beliefs between agents.
Due to the definition of Information and then with its relation to the probabilistic
correlation to truth/reality, we consider
information in relation with an agent's beliefs. Similarly, we consider
beliefs to define the actual behavior of an agent or a system. On the other hand,
Knowledge drives the nominal behavior of an agent or a system. This view
is not considered in most (if not all) the approaches to protocol verification
and the Dolev-Yao theory is usually applied to a representation/abstraction
of a protocol that only considers knowledge and transfer of knowledge (i.e.
if an agent sends a message, the recipient knows that message). However,
the security of a system is tightly related to the difference between
the nominal behavior and the implemented one.

\begin{definition}{\bf System State and State-Space --}\label{def:system}
	The state of a system (or a sub-system) $s$ is identified with an
	instance of an agent state (see Definition~\ref{def:agent}) such that
	$s=\langle\rcc(\knowledgeRegion,\beliefRegion),\rcc(\knowledgeRegion,\informationRegion),\rcc(\beliefRegion,\informationRegion)\rangle$,
	where $\knowledgeRegion,\beliefRegion,\informationRegion$ are Regions
	of Knowledge, Beliefs, and Information respectively, and $\rcc$ is a
	specific relation between the Regions it predicates on.  The
	state-space of a system is then defined by the different $\rcc$
	relations between the three pairs of Regions defining the system and
	all the sub-systems.
\end{definition}
A system state space can be seen as a superposition of multiple agent states where knowledge,
beliefs, and information are related between each other (in a mereotopological
space) through the all the admissible RCC relations. 
A \emph{system state} (as an instance of the system state space)
is a specific configuration of
the $\rcc$ relations between the three regions. Therefore, a collection of 
system states defines a collection of choices for those $\rcc$ relations.

% \item  ``normally concern the state or the history of the entire universe but only
% of some small part of
% it''.  
% \item  ``Information and probability are inversely related'', i.e. ``
% the more alternatives a proposition admits of, the more probable and the less
% informative it is, and vice versa''

\begin{figure}[t]
	\centering
	\includegraphics[width=.8\textwidth]{knowledge-belief.pdf}
	\caption{Informal representation of Knowledge and Belief}
	\label{fig:knowledge-belief}
\end{figure}

The difference between Knowledge and Belief is depicted in
Figure~\ref{fig:knowledge-belief} (see \autocite{wiki-knowledgebelief}).  However,
according to\autocite{Gettier2012knowledge},
Knowledge\footnote{``\emph{Theaetetus}: [\ldots] He said that knowledge was
true opinion accompanied by reason, but that unreasoning true opinion was
outside of the sphere of knowledge; and matters of which there is not a
rational explanation are unknowable -- yes, that is what he called them -- and
those of which there is are knowable. [\ldots] \emph{Socrates}: [\ldots] the
primary elements of which we and all else are composed admit of no rational
explanation; for each alone by itself can only be named, and no qualification
can be added, neither that it is nor that it is not, for that would at once be
adding to it existence or non-existence, whereas we must add nothing to it, if
we are to speak of that itself alone.  [\ldots]'' Plato -- Theaetetus 201
\autocite{Plato1914Plato}} as an epistemological concept is difficult to formally define. 
Similarly for the concept of information, Hintikka in \autocite{Hintikka1993Information}
states that ``A purely logical definition of information is impossible''.
In this work, however, we are not interested in how knowledge, information, or
belief can be precisely formalized from an epistemic standpoint.  We assume
that a semantic of a correct definition of epistemic knowledge exists, for
example the one given in\autocite{Hintikka1962knowledge} by Hintikka, and we
then define knowledge in terms of the Kripke structure defined in
Definition~\ref{def:modallogic}; similarly for Belief.

\begin{definition}{\bf Knowledge --}\label{def:knowledge}
Given an abstract collection of Agents $Ag$, and the modal operator
	$\knows{a}{}$ (where $a\in Ag$), Knowledge is defined as a region 
	of predicates known by an agent $\knowledge{a}=\bigcup_\Phi \knows{a}{\varphi}$ 
	(where $\Phi$ is the collection of all the propositions known by $a$).
	Given a proposition $P$, we extend the semantics of the Causality structure with:
	\begin{enumerate}[noitemsep]
		\item[$(\interpretation16)$] $\world\models\knows{a}{P}$ iff
			$\world'\models P$ for all $\world'$ such that
			$\world\modalrelation\world'$.
	\end{enumerate}
\end{definition}

\begin{definition}{\bf Belief --}\label{def:belief}
	Given an abstract collection of Agents $Ag$, and the modal operator
	$\believe{a}{}$ (where $a\in Ag$), Belief is defined as a region 
	of predicates believed by an agent $\belief{a}=\bigcup_\Phi \believe{a}{\varphi}$
	(where $\Phi$ is the collection of all the propositions believed by $a$).
	Given a proposition $P$, we extend the semantics of the Causality structure with:
	\begin{enumerate}[noitemsep]
		\item[$(\interpretation17)$] $\world\models\believe{a}{P}$ iff
			$\world\models\neg\knows{a}{\neg P}$ (i.e. the agent $a$ considers $P$ possible) 
			and $\world'\models P$ for all
			$\world'$ such that $\world\modalrelation\world'$.
	\end{enumerate}
\end{definition}

\begin{definition}{\bf Information --}\label{def:information}
	Given an abstract collection of Agents $Ag$, and the modal operator
	$\informs{a}{}$ (where $a\in Ag$), Information is defined as a region 
	of beliefs asserted by an agents $a$.
	%and transferred from $a$ to $b$ ($a\rightarrow b$).
	$\information{a}=\bigcup_\Phi \informs{a}{}{\varphi}$
	(where $\Phi$ is the region of all the propositions believed and asserted by $a$).
	Given a proposition $P$, we extend the semantics of the Causality structure with:
	\begin{enumerate}[noitemsep]
		\item[$(\interpretation18)$] $\world\models\informs{a}{P}$ iff
			$\world\models\belief{a}{P}$ (i.e. the agent $a$ considers $P$ possible), 
			$\world\models\neg\belief{a}{\neg P}$%, 
			%and $\world'\models P$ for all
			%$\world'$ such that $\world\modalrelation\world'$.
	\end{enumerate}
\end{definition}

\subsection{Engineering Operational System States}\label{sec:engsystemstate}
When dealing with a specification of a system (i.e. the design if the system
for a specific operation (as a generic operating/operational system),
considering abstract and general concepts such as knowledge, belief, or
information may result to be too abstract for the objectives of the overall engineering
process (e.g. verification and validation).
As an example, we are usually not
interested in information in abstract but to the transfer of that
information, which we call Assertions, made by a subset of agents. 
Therefore, we now define how we consider an agent for the engineering
of CPS.

\begin{enumerate}
	\item We consider Information only when the intention of exchanging 
		that Information is from a sender to 
		recipient is defined; and we call it \emph{Assertion}  
	\item Similarly, the portion of Beliefs we consider for system
		engineering is the one that builds (input) or describes
		(output) the behavior of an agent (strategy rules
		\footnote{``The logical structure of information is one of the
		most basic and one of the most basic and one of the simplest
		thing in the wide and wonderful world of logical analysis. This
		point can be put in a deeper perspective. A distinction
		[\ldots] ought to be made [--] between two kinds of rules (or
		principles) in any strategic activity like knowledge seeking.
		On the one hand you have the rules that define the game, e.g.
		how chessmen are moved on a board. The can be called
		\emph{definitory} rules.  They must be distinguished from rules
		[\ldots] that deal with what is better and what is worse in the
		game in question.  Definitory rules do not say anything about
		this subject. Rules which do can be called \emph{strategic
		rules}'' -- Hintikka in \autocite{Hintikka1993Information}}),
		and
	\item We consider a set of axiomatic \emph{Facts} (definitory rules) instead
		of considering the more general epistemic definition of
		Knowledge. Specifically, Facts describes:
		\begin{itemize}
			\item the functional architecture of each agents
			\item the structural (HW/SW) architecture of each
				subsystem of agents and agent within a
				subsystem
			\item the data flow as a transfer of Beliefs through
				Assertions (i.e. the Beliefs flow)
		\end{itemize}
\end{enumerate}

We give a graphical representation of (sub-)system and agent in Figure~\ref{fig:system-agent}.

\begin{figure}[t]
	\centering
	\includegraphics[width=.9\textwidth]{system-agent.pdf}
	\caption{Example of system and agent structure}
	\label{fig:system-agent}
\end{figure}

\begin{definition}{\bf Assertion -- }\label{def:assertion}
	An assertion is an intended transfer of beliefs between
	two agents $a$ and $b$ such that
	\begin{enumerate}[noitemsep]
		\item[$(\interpretation19)$] $\world\models\assert{a}{b}{P}$ iff
			$\world\models\belief{a}{P}$, 
			$\world'\models\rcc(\belief{a}{P},\belief{b}{P})$, 
			$\world'\models\belief{a}{\belief{b}{P}}$
			and $\world'\models P$ for all
			$\world'$ such that $\world\modalrelation\world'$.
	\end{enumerate}
\end{definition}

\begin{definition}{\bf Behavior -- }\label{def:behavior}
	The behavior of an agent is defined as the transformation process (e.g. defined as a
	protocol, or a functional architecture) that determined the
	output-beliefs based on input-beliefs (where input-beliefs and
	output-beliefs are beliefs taken as input or output respectively).
	\begin{enumerate}[noitemsep]
		\item[$(\interpretation21)$] $\world\models\bigcup_{s\in S}\believe{s}{P_{s}}$ and $\world\models\behave{a}{[\bigcup_{s\in S}\believe{s}{P_{s}}]}$ iff
			$\world\models\belief{a}{P'}$, 
			
		where $a$ is an agent, $S$ is a set of agents such that $a\neg\in S$, and the calculation of $P'$ is deteministic.
	\end{enumerate}
\end{definition}

\begin{definition}{\bf Fact -- }\label{def:fact}
Facts are a sub-region of Knowledge $\factRegion = \bigcup_\Phi \knows{a}{\varphi}$
	that predicate in a factive way, e.g.
	if $a$ knows that $P$ then $P$, and the region of facts is monotone (no revision).
	\begin{enumerate}[noitemsep]
		\item[$(\interpretation20)$] $\world\models\fact{a}{P}$ iff
			$\world \models \Box P$. 
	\end{enumerate}
\end{definition}

\begin{definition}{\bf Operational System State --}\label{def:system}
	An operational system (or a sub-system) is represented as an agent (see Definition~\ref{def:agent}) and then defined by the resulting state space,
	such that
	$s=\langle\rcc(\factRegion,\behaviorRegion),\rcc(\factRegion,\assertionRegion),\rcc(\behaviorRegion,\assertionRegion)\rangle$,
	where $\assertionRegion,\behaviorRegion,\factRegion$, are regions of assertions, behavior, and facts respectively.
\end{definition}

As already presented in \autocite{Santaca2016abf} it follows that, defining
a system (or an operational system) with a fixed number of regions, there exist
an upper-bound to the number of possible configuration of a system, defined by
the possible relations between the different regions.
For completeness, we report in the next paragraph 
the calculation done in \autocite{Santaca2016abf}.

\Paragraph{Number of different configurations of a system}
The general formula to calculate the number of different types of agents is
$r^{\binom{n}{k}}$, where $r$ is the number of relations with arity $k$,
between $n$ different sets, where $r^e$ is the number of permutation of $r$
relations over $e$ elements with repetitions, with $e$ being the number of
$k$-ary combinations of $n$ sets, $\binom{n}{k}$.
In our case, $\binom{n}{k}=3$ since we consider $3$ sets
($\assertionRegion,\behaviorRegion,\factRegion$), and all the relations
considered in the RCC are binary.  Hence, using RCC5 (with five different
spatial relations) over three sets, we can theoretically define up to 125
different type of agents. However, only 54 of the 125 (as showed in
\cite{improvingRCC}) combinations are topologically correct with respect to
the definition of the relations of RCC5. Generalizing to all the RCCs:

\begin{itemize}%[nosep]
\item \emph{RCC3} --- theoretical: $3^3=27$,  correct: 15 
\item \emph{RCC5} --- theoretical: $5^3=125$, correct: 54
\item \emph{RCC8} --- theoretical: $8^3=512$, correct: 193
\end{itemize}

Hence, even if considering a different number of sets than the three
$\assertionRegion$, $\behaviorRegion$ and $\factRegion$ exponentially affects
the number of theoretical agents, the application of RCC downscales that number
of a factor that ranges from 1.8 to 2.5. In addition, using RCC5 we consider
3.6 times more (different) types of agents than RCC3, but using RCC8 would
allow us to consider 3.5 times more different agents.

\subsection{System Security Evolution}\label{sec:systemevoution}
\fix{mr}{
Facts should be correlated to the causality principle. Therefore a
specification in opnet should be translated into vmt and and by bounded model
checking with nuxmv identify a path towards an undesired state, if that fails
use abf to mutate the first state into one of the possible according to abf and
re-run nuxmv.

Knowledge is the theory itself, an instance is the facts of a design of it. A
testing process tests facts against knowledge as much as we test a cps design
chiices against its specification.  Information carries the knowledge of the
negation of the opposite of the information. Therefore an assertion carries the
belief (as a result of the behavior execution) of not the opposite. But, as
Hintikka States in "popper" our previous statement is true if the probability
distribution of the information is representative wrt the causality structure
(i.e. knowledge and facts, see email before). So, based on our theory, the
assertions should have a number between 3 to 5 possible negated meaning, one
per each rcc if reference and not just a negation of the information. Those
negation representative of an insecure configuration on the communication of
the system. In turn, those configurations should categorize all the CWE.  The
fact that weaknesses are generated by mankind but not exploited makes malicious
attacks confined to the hackers (and black-hat hackers). Fix: move black-hat
hackers as subregion of mankind with malicious intent. Describe the intent as a
desire of reaching a breach of a cybersecurity property over an asset (agent of
the system), instead of searching for a specific attack. The tool would then
translate the system from opnet to abf formula. The abf tool generates all the
sat state of the system fix: add state to all def before this text. And we
shall define the causality relation between the different states generated by
the abf tool as chosen nondeterministically from the possible sat states by a
model checker such as nuxmv.

By calculating sat instead of validity with abf tool and then adding it to the
causality structure of a system fact description, e.g. in nuxmv; we can
calculate a path for reachability of an insecurity property violated on an
agent which represents an asset. Whenever nuxmv fails, we can add a new state
and re-run the process. Those guesses should be better managed by a trained
v-ai since the inference by induction from the partial information of the
system in which we represent the assertions as negation of a beliefs and
without rcc do not carry enough information for a Reasoner that doesn't
implemented rcc.
}

\subsection{Security Properties and Assets}\label{sec:properties}
In \autocite{Georgeoff1995bdi}, the BDI (Belief, Desire, Intent)
paradigm is explained; where an agent can be defined over those
three constituents.
